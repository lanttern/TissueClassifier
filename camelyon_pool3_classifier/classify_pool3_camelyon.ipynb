{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# All the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import cPickle as pickle\n",
    "import feature_calculations as fc\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the inception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_graph():\n",
    "  \"\"\"Creates a graph from saved GraphDef file and returns a saver.\"\"\"\n",
    "  # Creates graph from saved graph_def.pb.\n",
    "  with tf.gfile.FastGFile('../data/classify_image_graph_def.pb', 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    _ = tf.import_graph_def(graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load or compute pool3 for frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_dict = {} # dictionary to hold pool_3 features for each frame\n",
    "# the structure is :\n",
    "# my_dict = {fileID1: {label:'classification1',pool_3:[vector,of,pool3,1]},\n",
    "#            fileID2: {label:'classification2', pool_3:[vector,of,pool3,2]}}\n",
    "\n",
    "# first check if data already exists\n",
    "if os.path.exists('../data/camelyon_pool3.pkl'):\n",
    "    my_dict = pickle.load(open('../data/camelyon_pool3.pkl', 'rb'))\n",
    "\n",
    "# otherwise, we need to compute pool3 weights    \n",
    "else:\n",
    "    # create inceptions graph\n",
    "    create_graph()\n",
    "    sess = tf.InteractiveSession()\n",
    "    pool3_tensor = sess.graph.get_tensor_by_name('pool_3:0') # these are the weights we want!\n",
    "    for i in ['../data/slide_data/camelyon_metastatic/', '../data/slide_data/camelyon_normal/']:\n",
    "        # check that folder exists\n",
    "        if not os.path.isdir(i): print \"WARNING::Directory '\", i, \"' does not exist! Skipping...\"; continue\n",
    "        # get list of files\n",
    "        files = os.listdir(i)\n",
    "        for j,iFile in enumerate(files):\n",
    "            if (j%100==0): print \"working on file #\", j\n",
    "            if not '.jpeg' in iFile: continue # skip file that are not images\n",
    "            file_path = os.path.join(i,iFile) # full file path\n",
    "            img = cv2.imread(file_path) # load image for cv2\n",
    "            if float(fc.compute_white_area_1(img)) > 0.95: continue # same cleaning applied on 6 feature classifier\n",
    "            thisFrame = {} # dictionary for this frame\n",
    "            thisFrame['label'] = ('metastatic' in i)*'metastatic' + ('normal' in i)*'normal' # label\n",
    "            img_data = tf.gfile.FastGFile(file_path, 'rb').read() # load for tensorflow\n",
    "            thisFrame['pool3'] = np.squeeze(sess.run(pool3_tensor, {'DecodeJpeg/contents:0':img_data})) # features\n",
    "            file_ID = iFile.split('.')[1] #frame ID\n",
    "            my_dict[file_ID] = thisFrame\n",
    "    # save for later :)\n",
    "    pickle.dump( my_dict, open( \"../data/camelyon_pool3.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Use same test/train split as 6 feature classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# these were saved in the feature calculator\n",
    "setIDs = pickle.load(open('../data/train_cv_test_fileIDs.pkl', 'rb'))\n",
    "# subset into training, cv, test\n",
    "trainIDs = setIDs['train']\n",
    "cvIDs = setIDs['cv']\n",
    "testIDs = setIDs['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [x for x in trainIDs] gives a list of 1-d arrays.  need to grab the first element to get fileID\n",
    "trainX = [my_dict[x[0]]['pool3'] for x in trainIDs]\n",
    "trainY = [my_dict[x[0]]['label']=='metastatic' for x in trainIDs]\n",
    "cvX = [my_dict[x[0]]['pool3'] for x in cvIDs]\n",
    "cvY = [my_dict[x[0]]['label']=='metastatic' for x in cvIDs]\n",
    "testX = [my_dict[x[0]]['pool3'] for x in testIDs]\n",
    "testY = [my_dict[x[0]]['label'] for x in testIDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the length\n",
    "len(trainY), len(cvY), len(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the shape\n",
    "np.shape(trainX), np.shape(cvX), np.shape(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Let's look at the training set \n",
    "## Try first six PCA features (e.g. get a comparison to my engineered 6 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply the standard scaler for 0 mean, unit std\n",
    "scaler = preprocessing.StandardScaler().fit(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try just 6 PCs for comparison with feature classifier\n",
    "pca = PCA(n_components=6)\n",
    "pca = pca.fit(scaler.transform(trainX))\n",
    "# what % of variance is retained?\n",
    "sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform the data for plotting\n",
    "pca6_trainX = pca.transform(scaler.transform(trainX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to plot normalized PC distributions\n",
    "def plot_pca_component(component, dataX, dataY, nBins, title, xlabel, ylabel):\n",
    "    comp_index = component - 1 #PC1 has index 0\n",
    "    histogram = plt.figure()\n",
    "    bins = np.linspace(np.min(dataX), np.max(dataX), nBins) # define the binning\n",
    "    # normalize to unit area\n",
    "    plt.hist([dataX[x][comp_index] for x in range(len(dataX)) if not dataY[x]],\n",
    "             bins, weights=np.ones(len(dataY)-sum(dataY))/(len(dataY)-sum(dataY)),\n",
    "             alpha=0.5, label='normal',color=\"#00ff00\")\n",
    "    plt.hist([dataX[x][comp_index] for x in range(len(dataX)) if dataY[x]],\n",
    "             bins, weights=np.ones(sum(dataY))/(sum(dataY)),\n",
    "             alpha=0.5, label='metastatic',color=\"#990099\")\n",
    "    # labels and stuff\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    return histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make the labels bigger, plot pretty\n",
    "import seaborn\n",
    "seaborn.set(font_scale=1.9)\n",
    "seaborn.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "# plot the PCs\n",
    "for i in range(6):\n",
    "    plot_pca_component(i, pca.transform(scaler.transform(trainX)), trainY, 20, 'Principal Component '+str(i+1), 'arbitrary units', 'arbitrary units')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try logistic regression as a function of the number of PCs\n",
    "## ...and compare regularization parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dictionary to keep track score for different # PCs\n",
    "# {#PC1: score1, #PC2: score2}\n",
    "nPCAScoreDict = {}\n",
    "for i in range(1,500,10):\n",
    "    if (i-1)%100 == 0: print \"iteration\", i\n",
    "    pca = PCA(n_components=i) # new PCA each time\n",
    "    pca.fit(scaler.transform(trainX)) # fit the PCA\n",
    "    model = LogisticRegression(C=0.01).fit(pca.transform(scaler.transform(trainX)), trainY) # apply logistic regression\n",
    "    nPCAScoreDict[i] = model.score(pca.transform(scaler.transform(cvX)), cvY) # score logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# redo the above, but turn down amount of regulatization\n",
    "nPCAScoreDictNoReg = {}\n",
    "for i in range(1,500,10):\n",
    "    if (i-1)%100 == 0: print \"iteration\", i\n",
    "    pca = PCA(n_components=i)\n",
    "    pca.fit(scaler.transform(trainX))\n",
    "    model = LogisticRegression(C=30).fit(pca.transform(scaler.transform(trainX)), trainY)\n",
    "    nPCAScoreDictNoReg[i] = model.score(pca.transform(scaler.transform(cvX)), cvY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the cv score as a function of the number of PCs\n",
    "seaborn.set(font_scale=1.6)\n",
    "# scores from inception + logistic regression\n",
    "plt.scatter(nPCAScoreDict.keys(), nPCAScoreDict.values(), color='red', label='reg. param. = 0.01')\n",
    "plt.scatter(nPCAScoreDictNoReg.keys(), nPCAScoreDictNoReg.values(), color='blue', label='reg. param. = 30')\n",
    "# for reference, include scores from CV-based classifier\n",
    "plt.axhline(0.973, label = 'Computer vision + MLP Classifier', color='black')\n",
    "plt.xlabel('No. Principal Components')\n",
    "plt.ylabel('Cross-validation Accuracy')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the best score\n",
    "max(nPCAScoreDict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning curve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the cv score as a function of number of training data points\n",
    "nTrainScoreDict = {}\n",
    "for i in range(2, len(trainX), 20):\n",
    "    if (i-2)%1000 == 0: print \"i=\",i\n",
    "    nPCA = min(i,200) # can't have more PCs than number of training data points\n",
    "    # subset the training data\n",
    "    thisTrainX = trainX[:i]\n",
    "    thisTrainY = trainY[:i]\n",
    "    # need a new scaler \n",
    "    thisScaler = preprocessing.StandardScaler().fit(thisTrainX)\n",
    "    # do the pca / model fitting / model scoring\n",
    "    thisPCA = PCA(n_components=nPCA).fit(thisScaler.transform(thisTrainX))\n",
    "    model = LogisticRegression(C=0.01).fit(thisPCA.transform(thisScaler.transform(thisTrainX)),thisTrainY)\n",
    "    nTrainScoreDict[i] = model.score(thisPCA.transform(thisScaler.transform(cvX)), cvY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# repeat for no regularization\n",
    "nTrainScoreDictNoReg = {}\n",
    "for i in range(2, len(trainX), 20):\n",
    "    if (i-2)%1000 == 0: print \"i=\",i\n",
    "    nPCA = min(i,200)\n",
    "    thisTrainX = trainX[:i]\n",
    "    thisTrainY = trainY[:i]\n",
    "    thisScaler = preprocessing.StandardScaler().fit(thisTrainX)\n",
    "    thisPCA = PCA(n_components=nPCA).fit(thisScaler.transform(thisTrainX))\n",
    "    model = LogisticRegression(C=30).fit(thisPCA.transform(thisScaler.transform(thisTrainX)),thisTrainY)\n",
    "    nTrainScoreDictNoReg[i] = model.score(thisPCA.transform(thisScaler.transform(cvX)), cvY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plt.scatter(nTrainScoreDict.keys(), nTrainScoreDict.values(), color='red', label='reg. param. = 0.01')\n",
    "plt.scatter(nTrainScoreDictNoReg.keys(), nTrainScoreDictNoReg.values(), color='blue', label='reg. param. = 30')\n",
    "#plt.axhline(0.973, label = 'Computer vision + MLPClassifier', color='black')\n",
    "plt.xlabel('No. Training Examples')\n",
    "plt.ylabel('Cross-validation Accuracy')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open the dictionary of CVs scores as a function of number of training examples\n",
    "# from feature classifier\n",
    "mlpCVScore = pickle.load(open('../data/mlpCVScore.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot to compare the learning curves\n",
    "plt.scatter(nTrainScoreDict.keys(), nTrainScoreDict.values(), color='red', label='Inception-v3 + Log. Reg.')\n",
    "plt.scatter(mlpCVScore.keys(), mlpCVScore.values(), color='blue', label='Computer vision + MLPClassifier')\n",
    "#plt.axhline(0.973, label = 'Computer vision + MLPClassifier', color='black')\n",
    "plt.xlabel('No. Training Examples')\n",
    "plt.ylabel('Cross-validation Accuracy')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train a final model to evaluate precision / recall/ etc.\n",
    "nPCA = 200\n",
    "scaler = preprocessing.StandardScaler().fit(trainX)\n",
    "pca = PCA(n_components=nPCA).fit(thisScaler.transform(trainX))\n",
    "logReg = LogisticRegression(C=0.01).fit(thisPCA.transform(thisScaler.transform(trainX)),trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# other performance metrics\n",
    "tp = sum([logReg.predict(pca.transform(scaler.transform(cvX)))[x] == cvY[x] for x in range(len(cvY)) if cvY[x]])\n",
    "tn = sum([logReg.predict(pca.transform(scaler.transform(cvX)))[x] == cvY[x] for x in range(len(cvY)) if not cvY[x]])\n",
    "fn = sum([logReg.predict(pca.transform(scaler.transform(cvX)))[x] != cvY[x] for x in range(len(cvY)) if cvY[x]])\n",
    "fp = sum([logReg.predict(pca.transform(scaler.transform(cvX)))[x] != cvY[x] for x in range(len(cvY)) if not cvY[x]])\n",
    "#sum([model.predict(scaler.transform(cvX))[x] != cvY[x][0] for x in range(len(cvY)) if cvY[x][0]])\n",
    "#sum([model.predict(scaler.transform(cvX))[x] != cvY[x][0] for x in range(len(cvY)) if not cvY[x][0]])\n",
    "tp, tn, fp, fn, len(cvY)\n",
    "#precision = float(tp) / (tp+fp)\n",
    "#recall = float(tp) / (tp + fn)\n",
    "#f1 = 2.*float(precision*recall) / (precision + recall)\n",
    "#precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logReg.score(pca.transform(scaler.transform(cvX)), cvY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
