{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lots of imports, clean up later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import simple_df_tools as dft\n",
    "import feature_calculations as fc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib\n",
    "import seaborn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import itertools\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the features to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_dict = {'blobs_1':fc.calc_feature_1,\n",
    "                'blobs_2': fc.calc_feature_2,\n",
    "                'color_compactness':fc.color_compactness,\n",
    "                'avg_blue':fc.compute_avg_blue,\n",
    "                'avg_green':fc.compute_avg_green,\n",
    "                'avg_red':fc.compute_avg_red,\n",
    "                'tissue_discontinuity':fc.sift0_per_area\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for existing df, or start from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_df = pd.DataFrame()\n",
    "if os.path.exists('../data/camelyon_features.pkl'):\n",
    "    my_df = pd.read_pickle('../data/camelyon_features.pkl')\n",
    "else:\n",
    "    my_df = pd.read_pickle('../data/my_simple_empty_df.pkl')\n",
    "    for i in ['../data/slide_data/camelyon_metastatic/', '../data/slide_data/camelyon_normal/']:\n",
    "        if not os.path.isdir(i): print \"WARNING::Directory '\", i, \"' does not exist! Skipping...\"; continue\n",
    "        my_df = dft.add_slide(i, feature_dict, my_df)\n",
    "    my_df.to_pickle('../data/camelyon_features.pkl')\n",
    "\n",
    "# clean up frames that are majority white\n",
    "my_df = my_df.loc[my_df.blobs_1 >-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-transform for features peaked near zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_transform(dataframe, feature):\n",
    "    if not feature in dataframe.columns:\n",
    "        print \"WARNING::Feature not in dataframe, no transformation.\"\n",
    "        return dataframe\n",
    "    feature_min = np.min(dataframe.loc[dataframe[feature] > 0][feature])\n",
    "    dataframe[feature] = np.log(dataframe[feature] + 0.1*feature_min)\n",
    "    return dataframe\n",
    "my_df = log_transform(my_df, 'color_compactness')\n",
    "my_df = log_transform(my_df, 'tissue_discontinuity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get matrix of features and train/test split before plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# which features to use\n",
    "columns = ['blobs_2', 'color_compactness', 'tissue_discontinuity','avg_red','avg_green','avg_blue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfX = my_df.as_matrix(columns=columns)\n",
    "dfY = (my_df.as_matrix(columns=['classification']) == 'metastatic')\n",
    "dfID = (my_df.as_matrix(columns=['file_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set random seed\n",
    "random.seed(1)\n",
    "# ~ 60% of data for training\n",
    "randList = [random.random()<0.6 for x in range(len(dfX))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train and test\n",
    "trainX = np.array([dfX[x,:] for x in range(len(dfX)) if randList[x]])\n",
    "notTrainX = np.array([dfX[x,:] for x in range(len(dfX)) if not randList[x]])\n",
    "trainY = np.array([dfY[x,:] for x in range(len(dfX)) if randList[x]])\n",
    "notTrainY = np.array([dfY[x,:] for x in range(len(dfX)) if not randList[x]])\n",
    "trainID = np.array([dfID[x,:] for x in range(len(dfX)) if randList[x]])\n",
    "notTrainID = np.array([dfID[x,:] for x in range(len(dfX)) if not randList[x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cv and test\n",
    "cvX = np.array([notTrainX[x] for x in range(len(notTrainX)) if x%2==0])\n",
    "cvY = np.array([notTrainY[x] for x in range(len(notTrainX)) if x%2==0])\n",
    "testX = np.array([notTrainX[x] for x in range(len(notTrainX)) if x%2==1])\n",
    "testY = np.array([notTrainY[x] for x in range(len(notTrainX)) if x%2==1])\n",
    "cvFileID = np.array([notTrainID[x] for x in range(len(notTrainX)) if x%2==0])\n",
    "testFileID = np.array([notTrainID[x] for x in range(len(notTrainX)) if x%2==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffle the training data (for evaluating learning curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shufX, shufY, shufID = shuffle(trainX, trainY, trainID, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shufY = shufY.reshape(-1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the (training) data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting function\n",
    "def plot_feature(dataX, dataY, nBins, title, xlabel, ylabel):\n",
    "    histogram = plt.figure()\n",
    "    bins = np.linspace(np.min(dataX), np.max(dataX), nBins)\n",
    "    plt.hist([dataX[x] for x in range(len(dataX)) if not dataY[x]], bins, weights=np.ones(len(dataY)-sum(dataY))/(len(dataY)-sum(dataY)), alpha=0.5, label='normal')\n",
    "    plt.hist([dataX[x] for x in range(len(dataX)) if dataY[x]], bins, weights=np.ones(sum(dataY))/(sum(dataY)), alpha=0.5, label='metastatic')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    return histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# global font size (& others?)\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dictionary for labels :(\n",
    "labels = {'blobs_2':{'title': 'Nuclei Density', 'xlabel': 'Blobs / Active Area', 'ylabel':'arbitrary units'},\n",
    "         'color_compactness':{'title':'Color Compactness', 'xlabel':'log[Color Compactness / Active Area]', 'ylabel':'arbitrary units'},\n",
    "         'tissue_discontinuity':{'title':'Tissue Discontinuity', 'xlabel':'log[No. Edges / Active Area]', 'ylabel':'arbitrary units'},\n",
    "         'avg_red':{'title':'Average Pixel Red', 'xlabel':'<Red>', 'ylabel':'arbitrary units'},\n",
    "         'avg_green':{'title':'Average Pixel Green', 'xlabel':'<Green>', 'ylabel':'arbitrary units'},\n",
    "         'avg_blue':{'title':'Average Pixel Blue', 'xlabel':'<Blue>', 'ylabel':'arbitrary units'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, feature in enumerate(columns):\n",
    "    plot_feature(shufX[:,i], shufY, 20, labels[feature]['title'], \n",
    "                 labels[feature]['xlabel'], labels[feature]['ylabel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations among features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr = np.corrcoef(np.array(shufX.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note: corr is the matrix!\n",
    "column_names = [labels[x]['title'] for x in columns]\n",
    "seaborn.set(font_scale=1.5)\n",
    "map = seaborn.heatmap(corr, xticklabels=column_names, yticklabels=column_names,)\n",
    "map.set_xticklabels(column_names,rotation=90)\n",
    "map.set_title(\"Correlation Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start modeling! \n",
    "# First -- logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(shufX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# iterate over regularization parameters\n",
    "scoreDict = {}\n",
    "for c in [0.0001, 0.0003,0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 1000, 10000]:\n",
    "    logReg = LogisticRegression(C=c)\n",
    "    logReg = logReg.fit(scaler.transform(shufX), shufY)\n",
    "    score = logReg.score(scaler.transform(cvX), cvY)\n",
    "    scoreDict[c] = score\n",
    "max(scoreDict.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train a model with the optimal parameter(s)\n",
    "logReg = LogisticRegression(C=30)\n",
    "logReg = logReg.fit(scaler.transform(shufX), shufY)\n",
    "score = logReg.score(scaler.transform(cvX), cvY)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainScore = {}\n",
    "cvScore = {}\n",
    "for n in range(2,len(shufX)):\n",
    "    thisX = shufX[:n,:]\n",
    "    thisY = shufY[:n].reshape(-1,)\n",
    "    thisScaler = preprocessing.StandardScaler().fit(thisX)\n",
    "    thisLogReg = LogisticRegression(C=30)\n",
    "    thisLogReg.fit(thisScaler.transform(thisX), thisY)\n",
    "    trainScore[n] = thisLogReg.score(thisScaler.transform(thisX), thisY)\n",
    "    cvScore[n] = thisLogReg.score(thisScaler.transform(cvX), cvY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(trainScore.keys(), trainScore.values(), color='red', label='Train Accuracy')\n",
    "plt.scatter(cvScore.keys(), cvScore.values(), label='Cross Validation Accuracy')\n",
    "plt.xlabel('No. Training Examples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Logistic Regression Learning Curves')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# precision / recall\n",
    "tp = sum([logReg.predict(scaler.transform(cvX))[x] == cvY[x][0] for x in range(len(cvY)) if cvY[x][0]])\n",
    "tn = sum([logReg.predict(scaler.transform(cvX))[x] == cvY[x][0] for x in range(len(cvY)) if not cvY[x][0]])\n",
    "fp = sum([logReg.predict(scaler.transform(cvX))[x] != cvY[x][0] for x in range(len(cvY)) if cvY[x][0]])\n",
    "fn = sum([logReg.predict(scaler.transform(cvX))[x] != cvY[x][0] for x in range(len(cvY)) if not cvY[x][0]])\n",
    "#sum([model.predict(scaler.transform(cvX))[x] != cvY[x][0] for x in range(len(cvY)) if cvY[x][0]])\n",
    "#sum([model.predict(scaler.transform(cvX))[x] != cvY[x][0] for x in range(len(cvY)) if not cvY[x][0]])\n",
    "tp, tn, fp, fn, len(cvY)\n",
    "precision = float(tp) / (tp+fp)\n",
    "recall = float(tp) / (tp + fn)\n",
    "f1 = 2.*float(precision*recall) / (precision + recall)\n",
    "precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logReg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ypos = np.arange(len(columns))\n",
    "plt.bar(ypos, abs(logReg.coef_)[0])\n",
    "plt.xticks(ypos, column_names, rotation=30)\n",
    "plt.title('Feature Importance')\n",
    "plt.ylabel('|Log. Reg. Coef.|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's switch to a nonlinear classifier:\n",
    "## MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try 1 hidden layer and iterate over size\n",
    "params = [(x,) for x in range(2,12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlpScoreDict = {}\n",
    "for i in params:\n",
    "    for a in [0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300]:\n",
    "        mlp = MLPClassifier(solver='lbfgs', alpha=a, activation='relu', random_state=1, hidden_layer_sizes=i)\n",
    "        mlp.fit(scaler.transform(shufX), shufY)\n",
    "        mlpScoreDict[str(i[0])+'_'+str(a)] = mlp.score(scaler.transform(cvX), cvY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max(mlpScoreDict.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now add a second hidden layer\n",
    "params2 = [x for x in range(2,12)]\n",
    "params2 = [element for element in itertools.product([4],params2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in params2:\n",
    "    mlp = MLPClassifier(solver='lbfgs', alpha=0.01, activation='relu', random_state=1, hidden_layer_sizes=i)\n",
    "    mlp.fit(scaler.transform(shufX), shufY)\n",
    "    mlpScoreDict[str(i[0])+'_'+str(i[1])+'_'+str(0.01)] = mlp.score(scaler.transform(cvX), cvY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max(mlpScoreDict.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# One hidden layer is best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curve for MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlpTrainScore = {}\n",
    "mlpCVScore = {}\n",
    "for n in range(2,len(shufX),20):\n",
    "    if (n-2)%1000 == 0: print \"n=\",n\n",
    "    thisX = shufX[:n,:]\n",
    "    thisY = shufY[:n].reshape(-1,)\n",
    "    thisScaler = preprocessing.StandardScaler().fit(thisX)\n",
    "    thisMLP = MLPClassifier(solver='lbfgs', alpha=0.01, activation='relu', random_state=1, hidden_layer_sizes=(4,))\n",
    "    thisMLP.fit(thisScaler.transform(thisX), thisY)\n",
    "    mlpTrainScore[n] = thisMLP.score(thisScaler.transform(thisX), thisY)\n",
    "    mlpCVScore[n] = thisMLP.score(thisScaler.transform(cvX), cvY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(mlpTrainScore.keys(), mlpTrainScore.values(), color='red', label='Train Accuracy')\n",
    "plt.scatter(mlpCVScore.keys(), mlpCVScore.values(), label='Cross Validation Accuracy')\n",
    "plt.xlabel('No. Training Examples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('MLP Classifier Learning Curves')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='lbfgs', alpha=0.01, activation='relu', random_state=1, hidden_layer_sizes=(4,))\n",
    "mlp.fit(scaler.transform(shufX), shufY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# precision / recall\n",
    "tp = sum([mlp.predict(scaler.transform(cvX))[x] == cvY[x][0] for x in range(len(cvY)) if cvY[x][0]])\n",
    "tn = sum([mlp.predict(scaler.transform(cvX))[x] == cvY[x][0] for x in range(len(cvY)) if not cvY[x][0]])\n",
    "fp = sum([mlp.predict(scaler.transform(cvX))[x] != cvY[x][0] for x in range(len(cvY)) if cvY[x][0]])\n",
    "fn = sum([mlp.predict(scaler.transform(cvX))[x] != cvY[x][0] for x in range(len(cvY)) if not cvY[x][0]])\n",
    "#sum([model.predict(scaler.transform(cvX))[x] != cvY[x][0] for x in range(len(cvY)) if cvY[x][0]])\n",
    "#sum([model.predict(scaler.transform(cvX))[x] != cvY[x][0] for x in range(len(cvY)) if not cvY[x][0]])\n",
    "tp, tn, fp, fn, len(cvY)\n",
    "precision = float(tp) / (tp+fp)\n",
    "recall = float(tp) / (tp + fn)\n",
    "f1 = 2.*float(precision*recall) / (precision + recall)\n",
    "precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out model for later :)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joblib.dump(scaler, 'week4_demo_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joblib.dump(mlp, 'week4_demo_mlp_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
